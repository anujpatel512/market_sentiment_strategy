{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Collection - Stock Prices & News Headlines\n",
        "\n",
        "@author Anuj Patel\n",
        "\n",
        "This notebook covers:\n",
        "1. Downloading historical stock price data from Yahoo Finance\n",
        "2. Collecting news headlines for sentiment analysis\n",
        "3. Data preprocessing and storage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete!\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Create data directory if it doesn't exist\n",
        "Path('../data').mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Download Stock Price Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading price data for 10 stocks from 2022-01-01 to 2024-12-31...\n"
          ]
        }
      ],
      "source": [
        "# Define stock tickers - starting with major tech stocks\n",
        "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'NVDA', 'TSLA', 'NFLX', 'CRM', 'ADBE']\n",
        "\n",
        "# Date range for historical data\n",
        "start_date = \"2022-01-01\"\n",
        "end_date = \"2024-12-31\"\n",
        "\n",
        "print(f\"Downloading price data for {len(tickers)} stocks from {start_date} to {end_date}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  10 of 10 completed"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded data shape: (752, 50)\n",
            "Date range: 2022-01-03 00:00:00 to 2024-12-30 00:00:00\n",
            "Available data for 10 stocks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Download all data at once\n",
        "data = yf.download(tickers, start=start_date, end=end_date, group_by='ticker')\n",
        "\n",
        "if data is not None and not data.empty:\n",
        "    print(f\"Downloaded data shape: {data.shape}\")\n",
        "    print(f\"Date range: {data.index[0]} to {data.index[-1]}\")\n",
        "    print(f\"Available data for {len(tickers)} stocks\")\n",
        "else:\n",
        "    print(\"Failed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DATA STRUCTURE DEBUG ===\n",
            "Data shape: (752, 50)\n",
            "Column names: ['Ticker', 'Price']\n",
            "First few columns: [('TSLA', 'Open'), ('TSLA', 'High'), ('TSLA', 'Low'), ('TSLA', 'Close'), ('TSLA', 'Volume'), ('GOOGL', 'Open'), ('GOOGL', 'High'), ('GOOGL', 'Low'), ('GOOGL', 'Close'), ('GOOGL', 'Volume')]\n",
            "Sample ticker data columns: ['Open', 'High', 'Low', 'Close', 'Volume']\n",
            "Sample data:\n",
            "Price             Open        High         Low       Close     Volume\n",
            "Date                                                                 \n",
            "2022-01-03  174.542902  179.499559  174.425125  178.645630  104487900\n",
            "2022-01-04  179.254206  179.558473  175.809076  176.378357   99310400\n",
            "2022-01-05  176.290001  176.839648  171.411868  171.686691   94537600\n",
            "==================================================\n",
            "Saved AAPL: 752 rows\n",
            "Saved MSFT: 752 rows\n",
            "Saved GOOGL: 752 rows\n",
            "Saved AMZN: 752 rows\n",
            "Saved META: 752 rows\n",
            "Saved NVDA: 752 rows\n",
            "Saved TSLA: 752 rows\n",
            "Saved NFLX: 752 rows\n",
            "Saved CRM: 752 rows\n",
            "Saved ADBE: 752 rows\n",
            "\n",
            " Price data collection complete!\n"
          ]
        }
      ],
      "source": [
        "# Check if data download was successful\n",
        "if data is not None and not data.empty:\n",
        "    \n",
        "    # Debug: Check data structure first\n",
        "    print(\"=== DATA STRUCTURE DEBUG ===\")\n",
        "    print(f\"Data shape: {data.shape}\")\n",
        "    print(f\"Column names: {data.columns.names}\")\n",
        "    print(f\"First few columns: {list(data.columns[:10])}\")\n",
        "    print(f\"Sample ticker data columns: {list(data[tickers[0]].columns)}\")\n",
        "    print(\"Sample data:\")\n",
        "    print(data[tickers[0]].head(3))\n",
        "    print(\"=\" * 50)\n",
        "    # Save individual stock data with technical indicators\n",
        "    for ticker in tickers:\n",
        "        try:\n",
        "            # Extract data for this ticker\n",
        "            df = data[ticker].copy()\n",
        "            \n",
        "            # Add basic technical indicators (using Close since Adj Close not available)\n",
        "            df['Returns'] = df['Close'].pct_change()\n",
        "            df['Volume_MA_20'] = df['Volume'].rolling(20).mean()\n",
        "            df['Price_MA_20'] = df['Close'].rolling(20).mean()\n",
        "            df['Price_MA_5'] = df['Close'].rolling(5).mean()\n",
        "            df['Volatility_20'] = df['Returns'].rolling(20).std()\n",
        "            \n",
        "            # Simple RSI calculation\n",
        "            delta = df['Close'].diff()\n",
        "            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "            rs = gain / loss\n",
        "            df['RSI'] = 100 - (100 / (1 + rs))\n",
        "            \n",
        "            # Save to CSV\n",
        "            df.to_csv(f\"../data/{ticker}_price.csv\")\n",
        "            print(f\"Saved {ticker}: {len(df)} rows\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {ticker}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(\"\\n Price data collection complete!\")\n",
        "else:\n",
        "    print(\" Cannot process data - download failed or data is empty\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. News Headlines Collection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample headlines for AAPL (2 found):\n",
            "1. Entertainment\n",
            "2. New on Yahoo\n"
          ]
        }
      ],
      "source": [
        "def get_yahoo_headlines(ticker, max_headlines=10):\n",
        "    \"\"\"\n",
        "    Scrape recent headlines for a given stock ticker from Yahoo Finance\n",
        "    Note: This is a basic example - in production, you'd want to use proper APIs\n",
        "    \"\"\"\n",
        "    try:\n",
        "        url = f\"https://finance.yahoo.com/quote/{ticker}?p={ticker}\"\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        \n",
        "        # Find headline elements (this may need adjustment based on Yahoo's current structure)\n",
        "        headlines = []\n",
        "        for h in soup.find_all(['h3', 'h4'], limit=max_headlines * 2):\n",
        "            text = h.text.strip()\n",
        "            if text and len(text) > 10 and len(text) < 200:\n",
        "                headlines.append(text)\n",
        "                if len(headlines) >= max_headlines:\n",
        "                    break\n",
        "        \n",
        "        return headlines\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting headlines for {ticker}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Test the function\n",
        "sample_headlines = get_yahoo_headlines('AAPL', max_headlines=3)\n",
        "print(f\"Sample headlines for AAPL ({len(sample_headlines)} found):\")\n",
        "for i, headline in enumerate(sample_headlines):\n",
        "    print(f\"{i+1}. {headline}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting headlines for AAPL...\n",
            "Collecting headlines for MSFT...\n",
            "Collecting headlines for GOOGL...\n",
            "Collecting headlines for AMZN...\n",
            "Collecting headlines for META...\n",
            "Collecting headlines for NVDA...\n",
            "Collecting headlines for TSLA...\n",
            "Collecting headlines for NFLX...\n",
            "Collecting headlines for CRM...\n",
            "Collecting headlines for ADBE...\n",
            "\n",
            " Collected 12 headlines total\n",
            "Headlines per stock: {'AAPL': 2, 'MSFT': 2, 'NVDA': 2, 'TSLA': 2, 'NFLX': 2, 'ADBE': 2}\n",
            "Saved to: ../data/headlines_sample.csv\n"
          ]
        }
      ],
      "source": [
        "# Collect headlines for all tickers\n",
        "import time\n",
        "\n",
        "all_headlines = []\n",
        "\n",
        "for ticker in tickers:\n",
        "    print(f\"Collecting headlines for {ticker}...\")\n",
        "    headlines = get_yahoo_headlines(ticker, max_headlines=5)\n",
        "    \n",
        "    for headline in headlines:\n",
        "        all_headlines.append({\n",
        "            'ticker': ticker,\n",
        "            'headline': headline,\n",
        "            'date': datetime.datetime.now().strftime('%Y-%m-%d'),\n",
        "            'timestamp': datetime.datetime.now().isoformat(),\n",
        "            'source': 'yahoo_finance'\n",
        "        })\n",
        "    \n",
        "    # Be nice to the server\n",
        "    time.sleep(1)\n",
        "\n",
        "# Create DataFrame and save\n",
        "headlines_df = pd.DataFrame(all_headlines)\n",
        "headlines_df.to_csv('../data/headlines_sample.csv', index=False)\n",
        "\n",
        "print(f\"\\n Collected {len(headlines_df)} headlines total\")\n",
        "print(f\"Headlines per stock: {headlines_df['ticker'].value_counts().to_dict()}\")\n",
        "print(f\"Saved to: ../data/headlines_sample.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== SAMPLE HEADLINES ===\n",
            "  ticker       headline        date                   timestamp         source\n",
            "0   AAPL  Entertainment  2025-07-23  2025-07-23T17:54:18.041206  yahoo_finance\n",
            "1   AAPL   New on Yahoo  2025-07-23  2025-07-23T17:54:18.041212  yahoo_finance\n",
            "2   MSFT  Entertainment  2025-07-23  2025-07-23T17:54:19.580234  yahoo_finance\n",
            "3   MSFT   New on Yahoo  2025-07-23  2025-07-23T17:54:19.580238  yahoo_finance\n",
            "4   NVDA  Entertainment  2025-07-23  2025-07-23T17:54:25.879452  yahoo_finance\n",
            "5   NVDA   New on Yahoo  2025-07-23  2025-07-23T17:54:25.879458  yahoo_finance\n",
            "6   TSLA  Entertainment  2025-07-23  2025-07-23T17:54:27.738397  yahoo_finance\n",
            "7   TSLA   New on Yahoo  2025-07-23  2025-07-23T17:54:27.738401  yahoo_finance\n",
            "8   NFLX  Entertainment  2025-07-23  2025-07-23T17:54:29.390940  yahoo_finance\n",
            "9   NFLX   New on Yahoo  2025-07-23  2025-07-23T17:54:29.390945  yahoo_finance\n"
          ]
        }
      ],
      "source": [
        "# Display sample of collected headlines\n",
        "print(\"=== SAMPLE HEADLINES ===\")\n",
        "print(headlines_df.head(10))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
